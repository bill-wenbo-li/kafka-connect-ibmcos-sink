name=cos-sink
connector.class=com.ibm.eventstreams.connect.cossink.COSSinkConnector

# You can increase this for higher throughput
tasks.max=1

# The list of source Kafka topics (comma separated, no spaces)
topics=<TOPIC>

key.converter=org.apache.kafka.connect.storage.StringConverter
value.converter=org.apache.kafka.connect.storage.StringConverter


# (required) - API key used to connect to the Object Storage service instance.
cos.api.key=<COS_APIKEY>
# (required) - Location of the Object Storage service bucket, for example: eu-gb.
cos.bucket.location=<COS_LOCATION>
# (required) - Name of the Object Storage service bucket to write data into.
cos.bucket.name=<BUCKET_NAME>
# (required) - Resiliency of the Object Storage bucket. Must be one of: cross-region, regional, or single-site.
cos.bucket.resiliency=regional
# (optional) - Specify public to connect to the Object Storage service over the public internet, or private to connect from a connector running inside the IBM Cloud network, for example from an IBM Cloud Kubernetes Service cluster. The default is public.
cos.endpoint.visibility=public
# (optional) - The number of seconds (as measured wall clock time for the Connect Task instance) between reading the first record from Kafka, and writing all of the records read so far into an object storage object. This can be useful in situations where there are long pauses between Kafka records being produced to a topic, as it ensures that any records received by this connector will always be written into object storage within the specified period of time.
cos.object.deadline.seconds=5
# (optional) - The number of seconds (as measured by the timestamps in Kafka records) between reading the first record from Kafka, and writing all of the records read so far into an object storage object.
cos.object.interval.seconds=5
# (optional) - The maximum number of Kafka records to combine into a object.
cos.object.records=5
# (required) - CRN for the Object Storage service instance.
cos.service.crn=<COS_CRN>

# (optional) - Enable Parquet output.
cos.enable.parquet=false
# (optional) - IBM Cloud EventStream schema registry URL.
cos.schema.registry.url=<SCHEMA_REGISTRY_URL>
# (optional) - IBM Cloud EventStream API Key.
cos.schema.registry.apikey=<SCHEMA_REGISTRY_APIKEY>
# (optional) - Avro schema subject.
cos.schema.subject=<SCHEMA_SUBJECT>
# (optional) - Avro schema version.
cos.schema.version=<SCHEMA_VERSION>
# (optional) - Avro schema cache size.
cos.schema.cache.size=1000
# (optional) - Enable more elaborate support for Avro schema.
cos.enhanced.avro.schema.support=true
# (optional) - Parquet output buffer size, stored in bytes in memory.
cos.parquet.output.buffer.size=26214400
# (optional) - Parquet write mode, can be create or overwrite.
cos.parquet.write.mode=create
# (optional) - Compression codec, can be empty (uncompressed), snappy, gz, lzo, br, lz4, zstd.
cos.parquet.compression.codec=snappy
# (optional) - Row group size, a Parquet file consists of multiple row groups.
cos.parquet.row.group.size=268435356
# (optional) - Page size, a row group consists of multiple column chunks and each column chuck consists of multiple pages.
cos.parquet.page.size=65536
# (optional) - Enable dictionary encoding.
cos.parquet.dictionary.encoding=true
